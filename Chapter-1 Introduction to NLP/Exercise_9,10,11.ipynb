{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize,pos_tag,ne_chunk,sent_tokenize\n",
    "from nltk.wsd import lesk\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9.Treating named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are reading a book published by Packt which is based out of Birmingham.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"We are reading a book published by Packt which is based out of Birmingham.\"\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We', 'are', 'reading', 'a', 'book', 'published', 'by', 'Packt', 'which', 'is', 'based', 'out', 'of', 'Birmingham', '.']\n"
     ]
    }
   ],
   "source": [
    "# tokenization \n",
    "tokens = word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('are', 'VBP'), ('reading', 'VBG'), ('a', 'DT'), ('book', 'NN'), ('published', 'VBN'), ('by', 'IN'), ('Packt', 'NNP'), ('which', 'WDT'), ('is', 'VBZ'), ('based', 'VBN'), ('out', 'IN'), ('of', 'IN'), ('Birmingham', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# pos tagging \n",
    "pos = pos_tag(tokens)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tree.Tree'>\n",
      "(S\n",
      "  We/PRP\n",
      "  are/VBP\n",
      "  reading/VBG\n",
      "  a/DT\n",
      "  book/NN\n",
      "  published/VBN\n",
      "  by/IN\n",
      "  (PERSON Packt/NNP)\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  based/VBN\n",
      "  out/IN\n",
      "  of/IN\n",
      "  (GPE Birmingham/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# name entity recognition\n",
    "i = ne_chunk(pos)\n",
    "print(type(i))\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('PERSON', [('Packt', 'NNP')]), Tree('GPE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"keep your savings in the bank\"\n",
    "sentence_2 = \"Its so risky to drive over the banks of the road\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keep', 'your', 'savings', 'in', 'the', 'bank']\n"
     ]
    }
   ],
   "source": [
    "tokenize_1 = word_tokenize(sentence_1)\n",
    "print(tokenize_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Its', 'so', 'risky', 'to', 'drive', 'over', 'the', 'banks', 'of', 'the', 'road']\n"
     ]
    }
   ],
   "source": [
    "tokenize_2 = word_tokenize(sentence_2)\n",
    "print(tokenize_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('savings_bank.n.02')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meaning of bank in the first sentence \n",
    "lesk(tokenize_1,'bank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('bank.v.07')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# meaning of bank in the second sentence \n",
    "lesk(tokenize_2,'banks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visit [this](http://www.nltk.org/howto/wsd.html) and confirm that Synset('bank.v.07') means slope in the turn of a road"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Sentence Boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hey yo, what's up. Its me here. Your amigo. Namaste Bhrata. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hey yo, what's up.\", 'Its me here.', 'Your amigo.', 'Namaste Bhrata.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
